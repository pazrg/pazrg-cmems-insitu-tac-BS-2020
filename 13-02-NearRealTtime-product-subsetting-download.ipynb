{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![header](https://i.imgur.com/I4ake6d.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN SITU BLACK SEA TRAINING\n",
    "<div style=\"text-align: right\"><i> 13-02-Part-two-out-of-five </i></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BS `NRT` Product/dataset Subsetting & Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\">\n",
    "    <ul class=\"toc-item\">\n",
    "        <li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction\">Introduction</a></span></li>\n",
    "        <li>\n",
    "            <span><a href=\"#Setup\" data-toc-modified-id=\"Setup\">Setup</a></span>\n",
    "            <ul>\n",
    "                <li><span><a href=\"#Python-packages\" data-toc-modified-id=\"Python-packages\">Python packages</a></span></li>\n",
    "                <li><span><a href=\"#Copernicus-Database\" data-toc-modified-id=\"Copernicus-Database\">Copernicus database</a></span></li>\n",
    "                <li><span><a href=\"#Auxiliary-functions\" data-toc-modified-id=\"Setup\">Auxiliary functions</a></span></li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><span><a href=\"#Getting-started\" data-toc-modified-id=\"Getting-started\">Getting started</a></span></li>\n",
    "        <li><span><a href=\"#Operations\" data-toc-modified-id=\"Operations\">Operations</a></span>\n",
    "        <ul>\n",
    "            <li>\n",
    "            <span><a href=\"#Subsetting\" data-toc-modified-id=\"Subsetting\">Subsetting</a></span>\n",
    "            <ul>\n",
    "                <li><span><a href=\"#By-collection\" data-toc-modified-id=\"By-collection\">By collection</a></span></li>\n",
    "                <li><span><a href=\"#By-time-range\" data-toc-modified-id=\"By-time-range\">By time range</a></span></li>\n",
    "                <li><span><a href=\"#By-bounding-box\" data-toc-modified-id=\"By-bounding-box\">By bounding-box</a></span></li>\n",
    "                <li><span><a href=\"#By-last-position\" data-toc-modified-id=\"By-last-position\">By last position</a></span></li>\n",
    "                <li><span><a href=\"#By-data-type\" data-toc-modified-id=\"By-data-type\">By data type</a></span></li>\n",
    "                <li><span><a href=\"#By-file-type\" data-toc-modified-id=\"By-file-type\">By file type</a></span></li>\n",
    "                <li><span><a href=\"#By-parameter\" data-toc-modified-id=\"By-parameter\">By parameter</a></span></li>\n",
    "                <li><span><a href=\"#By-platform-code\" data-toc-modified-id=\"By-platform-code\">By platform code</a></span></li>\n",
    "                <li><span><a href=\"#By-provider\" data-toc-modified-id=\"By-provider\">By provider</a></span></li>\n",
    "                <li><span><a href=\"#Subsetting-by-several-criterias-at-once\" data-toc-modified-id=\"Subsetting-by-several-criterias-at-once\">By several criterias at once</a></span></li>\n",
    "            </ul>\n",
    "            </li>\n",
    "            <li><span><a href=\"#Exporting\" data-toc-modified-id=\"Exporting\">Exporting</a></span></li>\n",
    "            <li><span><a href=\"#Downloading\" data-toc-modified-id=\"Downloading\">Downloading</a></span></li>          \n",
    "            </ul>\n",
    "        </li>\n",
    "        </ul>\n",
    "        </li>\n",
    "        <li><span><a href=\"#Wrap-up\" data-toc-modified-id=\"Wrap-up\">Wrap-up</a></span></li>\n",
    "        <li><span><a href=\"#Next-tutorial\" data-toc-modified-id=\"Next-tutorial\">Next tutorial</a></span></li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "This notebook focus on selecting (subsetting) and downloading netCDF files from the available collections (`latest`, `monthly` and `history`) available within the In Situ Near Real Time product/dataset covering the Black Sea:`INSITU_BS_NRT_OBSERVATIONS_013_034`. Click [here](http://marine.copernicus.eu/services-portfolio/access-to-products/?option=com_csw&view=details&product_id=INSITU_BS_NRT_OBSERVATIONS_013_034) to  view the dedicated section of this product within the [CMEMS Catalog]('http://marine.copernicus.eu/services-portfolio/access-to-products/')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any In Situ NRT product/dataset is just a bunch of netcdf files produced by the platforms (*drifters, profilers, gliders, moorings, HF-radars, vessels etc*) deployed in a certain area (in this case, the Black Sea); so many files that navigation/subsetting can be challenging. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| ![BS.gif](img/BS.gif)| \n",
    "|:--:| \n",
    "| *GIF of the location (point or trajectory) of the platforms providing near real time data in the BS area since 1922* |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To smooth the process of data discovery to users, the In Situ TAC provides a set of `index files` that describe the aforementioned netCDF collections. In addition to these index files, it is also possible to find further info regarding each platform contributting with files on the `index_platform.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Index |  Description |\n",
    "| ------- | ----------- |\n",
    "| `index_latest.txt`  |  List of available files within the latest collection + metadata | \n",
    "| `index_monthly.txt`   | List of available files within the monthly collection + metadata |\n",
    "| `index_history.txt`   |  List of available files within the history collection + metadata |\n",
    "| `index_platform.txt`   | Full list of platforms + metadata |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check more about these files in the <a href=\"https://archimer.ifremer.fr/doc/00324/43494/\" target=\"_blank\">Product User Manual</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>OBJECTIVE</b>\n",
    "    \n",
    "***  \n",
    "To select and download only those netCDFs matching our needs from the whole original set of files that composes the BS NRT product/dataset by using the aforementioned index files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the notebook to properly run we need to first load the next packages available from the Jupyter Notebook Ecosystem. Please run the `next cell`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import IPython\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import random\n",
    "import os\n",
    "from collections import namedtuple\n",
    "import ftputil\n",
    "from shapely.geometry import box, Point\n",
    "from urllib.parse import urlparse\n",
    "import folium\n",
    "from folium import plugins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>WARNING</b>\n",
    "    \n",
    "***  \n",
    "If any of them raises any error it means you need to install the module first. For doing so please:\n",
    "1. Open a new cell int he notebook\n",
    "2. Run <i>`!conda install packageName --yes`</i> or <i>`!conda install -c conda-forge packageName --yes`</i> or <i>`!pip install packageName`</i>\n",
    "3. Import again!\n",
    "<br><br>\n",
    "Example: <i>how-to-solve import error for json2html module </i>\n",
    "\n",
    "![region.png](img/errorImporting.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copernicus Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please `set next your CMEMS User credentials` in the next cell and `run the cell` afterwards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr = 'inputHereYourCMEMSUser'\n",
    "pas = 'inputHereYourCMEMSPassword'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>WARNING</b>\n",
    "    \n",
    "***  \n",
    "**Don't you have credentials yet?** <br>Please go [here](https://resources.marine.copernicus.eu/?option=com_sla) to get the above credentials to be able to access CMEMS secured FTP server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated before, we will focus on the Near Real Time product/dataset covering the Iberian-Biscay-Ireland seas so, please `run the next` to load the info defining such product/dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    'host': 'nrt.cmems-du.eu',#ftp host => nrt.cmems-du.eu for Near Real Time products\n",
    "    'product': 'INSITU_BS_NRT_OBSERVATIONS_013_034',#name of the In Situ Near Real Time product in the BS area\n",
    "    'name': 'bs_multiparameter_nrt',#name of the dataset available in the above In Situ Near Real Time product\n",
    "    'index_files': ['index_latest.txt', 'index_monthly.txt', 'index_history.txt'],#files describing the content of the lastest, monthly and history netCDF file collections available withint he above dataset\n",
    "    'index_platform': 'index_platform.txt',#files describing the netwotk of platforms contributting with files in the abve collections\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>TIP</b>\n",
    "    \n",
    "***  \n",
    "In case you want to explore any other In Situ NRT product/dataset just set the above definitions accordingly and you will be able to reproduce the subsetting and downloading we will perform next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For exploring the product/dataset we will use a set of files called `index files` located in the CMEMS FTP server. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Getting the linsk to the index files**.\n",
    "In order to get the links to download manualy the most updated version of these files we will use the next function. <br>Please `run the next cell` to load it in memory for later use: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIndexFileLinks(usr,pas,dataset):\n",
    "    #Provides the link to download each index file from the ftp server\n",
    "    url = os.path.join('ftp://', dataset['host'],'Core',dataset['product'],dataset['name'])\n",
    "    indexes = dataset['index_files'] + [dataset['index_platform']]\n",
    "    for index in indexes:\n",
    "        index_url = os.path.join(url,index)\n",
    "        urlParsed = urlparse(index_url)\n",
    "        index_url = index_url.replace(urlParsed.netloc, usr + ':' + pas + '@' + urlParsed.netloc)\n",
    "        print('...Click and download '+index+' from:')\n",
    "        print(index_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Index files reader**.\n",
    "In order to load the information contained in *each* of the files we will use the next function. <br>Please `run the next cell` to load it in memory for later use: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readIndexFileFromCWD(path2file):\n",
    "    #Load as pandas dataframe the file in the provided path\n",
    "    with open(path2file, 'rb') as f:\n",
    "        filename = os.path.basename(path2file)\n",
    "        print('...Loading info from: '+filename)\n",
    "        raw_index_info = pd.read_csv(path2file, skiprows=5)\n",
    "    return raw_index_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Index files merger**.\n",
    "In order to load the information contained in *all* the indexes in a one single entity we will use the next function. <br>Please `run the next cell` to load it in memory for later use: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIndexFilesInfo(usr, pas, dataset_info):\n",
    "    # Load and merge in a single entity all the information contained on each file descriptor of a given dataset\n",
    "    # 1) Loading the index platform info as dataframe\n",
    "    path2file = os.path.join(os.getcwd(),'data', 'index_files', dataset['index_platform'])\n",
    "    indexPlatform = readIndexFileFromCWD(path2file)\n",
    "    indexPlatform.rename(columns={indexPlatform.columns[0]: \"platform_code\" }, inplace = True)\n",
    "    indexPlatform = indexPlatform.drop_duplicates(subset='platform_code', keep=\"first\")\n",
    "    # 2) Loading the index files info as dataframes\n",
    "    netcdf_collections = []\n",
    "    for filename in dataset['index_files']:\n",
    "        path2file = os.path.join(os.getcwd(),'data', 'index_files',filename)\n",
    "        indexFile = readIndexFileFromCWD(path2file)\n",
    "        netcdf_collections.append(indexFile)\n",
    "    netcdf_collections = pd.concat(netcdf_collections)\n",
    "    # 3) creating new columns: derived info\n",
    "    netcdf_collections['netcdf'] = netcdf_collections['file_name'].str.split('/').str[-1]\n",
    "    netcdf_collections['file_type'] = netcdf_collections['netcdf'].str.split('.').str[0].str.split('_').str[1]\n",
    "    netcdf_collections['data_type'] = netcdf_collections['netcdf'].str.split('.').str[0].str.split('_').str[2]\n",
    "    netcdf_collections['platform_code'] = netcdf_collections['netcdf'].str.split('.').str[0].str.split('_').str[3]\n",
    "    #4) Merging the information of all files\n",
    "    headers = ['platform_code','wmo_platform_code', 'institution_edmo_code', 'last_latitude_observation', 'last_longitude_observation','last_date_observation']\n",
    "    result = pd.merge(netcdf_collections,indexPlatform[headers],on='platform_code')\n",
    "    print('Ready!')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's load also the next functions as the are to be used later in the subsetting processing: `run the next cells`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul><li id=\"TemporalOverlap\"><i>Time-Overlap</i> function definition</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeOverlap(row, targeted_range):\n",
    "    # Checks if a file contains data in the specified time range (targeted_range)\n",
    "    date_format = \"%Y-%m-%dT%H:%M:%SZ\"\n",
    "    targeted_ini = datetime.datetime.strptime(targeted_range.split('/')[0], date_format)\n",
    "    targeted_end = datetime.datetime.strptime(targeted_range.split('/')[1], date_format)\n",
    "    time_start = datetime.datetime.strptime(row['time_coverage_start'],date_format)\n",
    "    time_end = datetime.datetime.strptime(row['time_coverage_end'],date_format)\n",
    "    Range = namedtuple('Range', ['start', 'end'])\n",
    "    r1 = Range(start=targeted_ini, end=targeted_end)\n",
    "    r2 = Range(start=time_start, end=time_end)\n",
    "    latest_start = max(r1.start, r2.start)\n",
    "    earliest_end = min(r1.end, r2.end)\n",
    "    delta = (earliest_end - latest_start).days + 1\n",
    "    overlap = max(0, delta)\n",
    "    if overlap != 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul><li id=\"SpatialOverlap\"><i>Spatial-Overlap</i> function definition</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatialOverlap(row, targeted_bbox):\n",
    "    # Checks if a file contains data in the specified area (targeted_bbox)\n",
    "    geospatial_lat_min = float(row['geospatial_lat_min'])\n",
    "    geospatial_lat_max = float(row['geospatial_lat_max'])\n",
    "    geospatial_lon_min = float(row['geospatial_lon_min'])\n",
    "    geospatial_lon_max = float(row['geospatial_lon_max'])\n",
    "    targeted_bounding_box = box(targeted_bbox[0], targeted_bbox[1],targeted_bbox[2], targeted_bbox[3])\n",
    "    bounding_box = box(geospatial_lon_min, geospatial_lat_min,geospatial_lon_max, geospatial_lat_max)\n",
    "    if targeted_bounding_box.intersects(bounding_box):  # check other rules on https://shapely.readthedocs.io/en/stable/manual.html\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul><li id=\"LocationInRange\"><i>LocationInRange</i> function definition</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lastLocationInRange(row, targeted_bbox):\n",
    "    # Checks if a file has been produced by a platform whose last position is within the specified area (targeted_bbox)\n",
    "    geospatial_lat = float(row['last_latitude_observation'])\n",
    "    geospatial_lon = float(row['last_longitude_observation'])\n",
    "    targeted_bounding_box = box(targeted_bbox[0], targeted_bbox[1],targeted_bbox[2], targeted_bbox[3])\n",
    "    location = Point(geospatial_lon, geospatial_lat)\n",
    "    if targeted_bounding_box.contains(location):#check other rules on https://shapely.readthedocs.io/en/stable/manual.html\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the subsetting to be carried out we need the most recent version of the index files. In `/data/index_files` we have provided a copy but, **if you are running this notebook later than April 2020** please:<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download again the index files. `Run the next cell` to dicover the links to download the index files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getIndexFileLinks(usr,pas,dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Upload to the `/data/index_files` folder the downloaded files. <br> `Run the next cell` to do it from this very same notebook currently opened:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.IFrame('data/index_files', width='100%', height=350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load now the info contained in such files by `running the next cell!`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = getIndexFilesInfo(usr, pas, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Run now the next cell` to see the information just loaded above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important fields in the above info is the `file_name`. This filed contains the full path to the file in the FTP server, saving users from having to know the actual FTP structure. As the above preview does not render such field completely, let's see the first file full path: `Run he next cell`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info['file_name'].tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the path into the browser....the file will download straightaway!<br>\n",
    "Next we will aim:\n",
    "<ul><li>the `file_name` field (file path) for downloading operations</li>\n",
    "    <li> the other fileds (file metadata) for the subsetting operations</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to <i>download</i> we need to select only those netCDF files that are of our interest.<br>There are many different criterias (if they contain a certain parameter, if they have covered a certain area, if they have data in a specific time range....).<br>Next we will see some examples..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This restrict all available files to just one specific collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>IMPORTANT</b>\n",
    "\n",
    "***\n",
    "Please remember available options:\n",
    "<ul>\n",
    "    <li><i>latest</i>: daily files from platforms (last 30 days of data)</li>\n",
    "    <li><i>monthly</i>: monthly files from platforms (last 5 years of data)</li>\n",
    "    <li><i>history</i>: one file per platform (all platform data)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Set one collecion` next and `run the cells`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targeted_collection = 'history' #try 'latest', 'monthly' or 'history'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = info['file_name'].str.contains(targeted_collection)\n",
    "subset = info[condition]\n",
    "subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"\">\n",
    "<b>TIP</b>\n",
    "    \n",
    "***  \n",
    "To check if the above output only list the files coming from a certain collection (the one set as targeted_collection') just check the `netcdf` column\n",
    "<ul><li>'latest' timestamp: netcdf file name ends with a YYYYMMDD</li></ul>\n",
    "<ul><li>'monthly' timestamp: netcdf file name ends with a YYYYMM</li></ul>\n",
    "<ul><li>'history' timestamp: netcdf file name ends with a YYYY or none timestamp</li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By time range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select only the files containing data within the next range of dates. <br>Please `set the start/end dates` you are interested in `and run the cells bellow`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targeted_range = '2018-01-01T00:00:00Z/2019-01-01T23:59:59Z' #set your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info['timeOverlap'] = info.apply(timeOverlap,targeted_range=targeted_range,axis=1)\n",
    "condition = info['timeOverlap'] == True\n",
    "subset = info[condition]\n",
    "subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>TIP</b>\n",
    "    \n",
    "***  \n",
    "To check if the above output only list the files containing data within a certain time range (the one set as 'targeted_range') just check `time_coverage_start` and `time_coverage_end` columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  By bounding-box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look for files containing data from a specific area.<br>`Please set your own area limits` next `and run the cell`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targeted_geospatial_lat_min = 43.0  # enter min latitude of your bounding box\n",
    "targeted_geospatial_lat_max = 45.0  # enter max latitude of your bounding box\n",
    "targeted_geospatial_lon_min = 28.0  # enter min longitude of your bounding box\n",
    "targeted_geospatial_lon_max = 30.0  # enter max longitude of your bounding box\n",
    "targeted_bbox = [targeted_geospatial_lon_min, targeted_geospatial_lat_min, targeted_geospatial_lon_max, targeted_geospatial_lat_max]  # (minx, miny, maxx, maxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the area you have set before: `run the next cell`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(location=[39.0, 0], zoom_start=4)\n",
    "upper_left = [targeted_geospatial_lat_max, targeted_geospatial_lon_min]\n",
    "upper_right = [targeted_geospatial_lat_max, targeted_geospatial_lon_max]\n",
    "lower_right = [targeted_geospatial_lat_min, targeted_geospatial_lon_max]\n",
    "lower_left = [targeted_geospatial_lat_min, targeted_geospatial_lon_min]\n",
    "edges_ = [upper_left, upper_right, lower_right, lower_left]\n",
    "m.add_child(folium.vector_layers.Polygon(locations=edges_))\n",
    "#Zooming closer\n",
    "m.fit_bounds(edges_, max_zoom=5)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>WARNING</b>\n",
    "    \n",
    "***  \n",
    "If you do not see any map when running the next cell please change your navigator (try chrome!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Run the next cell` to obtain the subset of files with data in such area:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "info['spatialOverlap'] = info.apply(spatialOverlap,targeted_bbox=targeted_bbox,axis=1)\n",
    "condition = info['spatialOverlap'] == True\n",
    "subset = info[condition]\n",
    "subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check now if the bbox of the above files truely overlaps with the targeted area!: `run the next cells`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfFiles = 800 #we will check just a sample of files not all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(location=[39.0, 0], zoom_start=6)\n",
    "m.add_child(folium.vector_layers.Polygon(locations=edges_))\n",
    "for platform, files in subset[:numberOfFiles].groupby(['platform_code', 'data_type']):\n",
    "    color = \"%06x\" % random.randint(0, 0xFFFFFF)\n",
    "    for i in range(0, len(files)):\n",
    "        netcdf = files.iloc[i]['file_name'].split('/')[-1]\n",
    "        upper_left = [\n",
    "            files.iloc[i]['geospatial_lat_max'],\n",
    "            files.iloc[i]['geospatial_lon_min']\n",
    "        ]\n",
    "        upper_right = [\n",
    "            files.iloc[i]['geospatial_lat_max'],\n",
    "            files.iloc[i]['geospatial_lon_max']\n",
    "        ]\n",
    "        lower_right = [\n",
    "            files.iloc[i]['geospatial_lat_min'],\n",
    "            files.iloc[i]['geospatial_lon_max']\n",
    "        ]\n",
    "        lower_left = [\n",
    "            files.iloc[i]['geospatial_lat_min'],\n",
    "            files.iloc[i]['geospatial_lon_min']\n",
    "        ]\n",
    "        edges = [upper_left, upper_right, lower_right, lower_left]\n",
    "        popup_info = '<b>netcdf</b>: ' + files.iloc[i]['netcdf']\n",
    "        m.add_child(folium.vector_layers.Polygon(locations=edges,color='#' + color,popup=(folium.Popup(popup_info))))\n",
    "        m.fit_bounds(edges, max_zoom=6)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>WARNING</b>\n",
    "    \n",
    "***  \n",
    "If you do not see any map when running the next cell please change your navigator (try chrome!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>TIP</b>\n",
    "    \n",
    "***  \n",
    "To check if everything went well, just check if the files bbox is indeed at some point within the one you were interested in (targeted bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>IMPORTANT</b>\n",
    "    \n",
    "***  \n",
    "If you are not satisfied with the resulting output (i.e you want to get only the files whose bbox is completely within the targeted area) please revisit the [definition of the SpatialOverlap](#SpatialOverlap) function and replace the rule applied (`intsersecs`) by any of the avilable ones [here](https://shapely.readthedocs.io/en/stable/manual.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By last position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look for files produced by a platform whose last position is within a specific area.<br>`Please set your own area limits` next `and run the cell`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targeted_geospatial_lat_min = 45.0  # enter min latitude of your bounding box\n",
    "targeted_geospatial_lat_max = 48.0  # enter max latitude of your bounding box\n",
    "targeted_geospatial_lon_min = 34.5  # enter min longitude of your bounding box\n",
    "targeted_geospatial_lon_max = 40.0  # enter max longitude of your bounding box\n",
    "targeted_bbox = [targeted_geospatial_lon_min, targeted_geospatial_lat_min, targeted_geospatial_lon_max, targeted_geospatial_lat_max]  # (minx, miny, maxx, maxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the area you have set before: `run the next cell`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(location=[39.0, 0], zoom_start=4)\n",
    "upper_left = [targeted_geospatial_lat_max, targeted_geospatial_lon_min]\n",
    "upper_right = [targeted_geospatial_lat_max, targeted_geospatial_lon_max]\n",
    "lower_right = [targeted_geospatial_lat_min, targeted_geospatial_lon_max]\n",
    "lower_left = [targeted_geospatial_lat_min, targeted_geospatial_lon_min]\n",
    "edges_ = [upper_left, upper_right, lower_right, lower_left]\n",
    "m.add_child(folium.vector_layers.Polygon(locations=edges_))\n",
    "#Zooming closer\n",
    "m.fit_bounds(edges_, max_zoom=5)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>WARNING</b>\n",
    "    \n",
    "***  \n",
    "If you do not see any map when running the next cell please change your navigator (try chrome!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Run the next cell` to obtain the subset of files with data in such area:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info['lastLocationInRange'] = info.apply(lastLocationInRange,targeted_bbox=targeted_bbox,axis=1)\n",
    "condition = info['lastLocationInRange'] == True\n",
    "subset = info[condition]\n",
    "subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check now if the bbox of the above files truely overlaps with the targeted area!: `run the next cells`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfFiles = 200 #we will check just a sample of files not all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(location=[39.3, 0], zoom_start=5)\n",
    "m.add_child(folium.vector_layers.Polygon(locations=edges_))\n",
    "for platform, files in subset[:numberOfFiles].groupby(['platform_code', 'data_type']):\n",
    "    color = \"%06x\" % random.randint(0, 0xFFFFFF)\n",
    "    for i in range(0, len(files)):\n",
    "        #Last reported position to map as marker\n",
    "        m.add_child(folium.Marker([files.iloc[i]['last_latitude_observation'], files.iloc[i]['last_longitude_observation']], popup=files.iloc[i]['platform_code']+' last position' ))\n",
    "#Zooming closer\n",
    "m.fit_bounds(edges, max_zoom=5)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>WARNING</b>\n",
    "    \n",
    "***  \n",
    "If you do not see any map when running the next cell please change your navigator (try chrome!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>TIP</b>\n",
    "    \n",
    "***  \n",
    "To check if everything went well, just check if the markers (last platform position) is indeed within the one you were interested in (targeted bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>IMPORTANT</b>\n",
    "    \n",
    "***  \n",
    "If you are not satisfied with the resulting output please revisit the [definition of the lastLocationInRange](#LocationInRange) function and replace the rule applied (`contains`) by any of the avilable ones [here](https://shapely.readthedocs.io/en/stable/manual.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look for files produced by a certain data type.<br>`Please set a data type` next `and run the cell`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targeted_data_type = 'PF' # try others: TG for Tide Gauges, PF for profilers etc =>Product User Manual: https://archimer.ifremer.fr/doc/00324/43494/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Run the next cell` to obtain just the files reported by such data type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = info['data_type'] == targeted_data_type\n",
    "subset = info[condition]\n",
    "subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>TIP</b>\n",
    "    \n",
    "***  \n",
    "To check if the subset only contains the aimed data type (targeted_data_type) just check the `data_type` column of the above output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By file type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look for certain types of files.<br>`Please set a file type` next `and run the cell`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targeted_file_type = 'PR' # try others: TS for Time Series...=>Product User Manual: https://archimer.ifremer.fr/doc/00324/43494/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Run the next cell` to obtain just the above type of files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = info['file_type'] == targeted_file_type\n",
    "subset = info[condition]\n",
    "subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>TIP</b>\n",
    "    \n",
    "***  \n",
    "To check if the subset matches the aimed file type (targeted_file_type) just check the `file_type` column of the above output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look for files containing a certain parameter.<br>`Please set a parameter code` next `and run the cell`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targeted_parameter = 'PSAL' #try others: TEMP, SLEV etc => In Situ parameter list: https://archimer.ifremer.fr/doc/00422/53381/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to obtain just the files reporting such parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = info['parameters'].str.contains(targeted_parameter)\n",
    "subset = info[condition]\n",
    "subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>TIP</b>\n",
    "    \n",
    "***  \n",
    "To check if the subset only contains the aimed parameter (targeted_parameter) just check the `parameters` column of the above output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By platform code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look for files produced by a certain platform.<br>`Please set a platform code` next `and run the cell`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targeted_platform_code = 'Constanta'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to obtain just the files reported by such platform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = info['platform_code']==targeted_platform_code\n",
    "subset = info[condition]\n",
    "subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>TIP</b>\n",
    "    \n",
    "***  \n",
    "To check if the subset matches the aimed platform (targeted_platform_code) just check the `platform_code` column of the above output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By provider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look for files produced by a certain provider.<br>`Please set a provider code` next `and run the cell`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targeted_provider_code = '850'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to obtain just the files reported by such platform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info['institution_edmo_code'] = ' '+info['institution_edmo_code']+' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = info['institution_edmo_code'].str.contains(targeted_provider_code, na=False)\n",
    "subset = info[condition]\n",
    "subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>TIP</b>\n",
    "    \n",
    "***  \n",
    "To check if the subset matches the aimed provider (targeted_provider_code) just check the `insitution_edmo_code` column of the above output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subsetting by several criterias at once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the collection of interest, range of time and bbox to find only the files that matches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targeted_collection = 'history'\n",
    "targeted_range = '2019-01-01T00:00:00Z/2019-12-01T23:59:59Z'\n",
    "targeted_data_type = 'TS'\n",
    "targeted_geospatial_lat_min = 40.0  # enter min latitude of your bounding box\n",
    "targeted_geospatial_lat_max = 48.0  # enter max latitude of your bounding box\n",
    "targeted_geospatial_lon_min = 28.0  # enter min longitude of your bounding box\n",
    "targeted_geospatial_lon_max = 43.0  # enter max longitude of your bounding box\n",
    "targeted_bbox = [targeted_geospatial_lon_min, targeted_geospatial_lat_min, targeted_geospatial_lon_max, targeted_geospatial_lat_max]  # (minx, miny, maxx, maxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to apply yhe above filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info['timeOverlap'] = info.apply(timeOverlap,targeted_range=targeted_range,axis=1)\n",
    "condition1 = info['timeOverlap'] == True\n",
    "info['spatialOverlap'] = info.apply(spatialOverlap,targeted_bbox=targeted_bbox,axis=1)\n",
    "condition2 = info['spatialOverlap'] == True\n",
    "\n",
    "condition3 = info['data_type'] == targeted_data_type\n",
    "condition4 = info['file_name'].str.contains(targeted_collection)\n",
    "subset = info[condition1 & condition2 & condition3 & condition4]\n",
    "subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you just want to export the above table containing ftp link to download each file in subset and some more metadata as excel for sharing it (a lot more compact than sharing the actual files), just run the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.to_excel('subsetOffiles.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have created your own subset (see above examples about how-to), we will loop over the files in such subset and download each of them from the FTP server thanks to the `file_name` column, field that contains the ftp link to the file.<br> Run the next cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = os.getcwd()# Defaults to the current working directory; change it as you please"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ftputil.FTPHost(dataset['host'], usr, pas) as ftp_host:  # connect to CMEMS FTP\n",
    "    for i in range(0, len(subset)):\n",
    "        filepath = subset.iloc[i]['file_name'].split(dataset['host'])[1]\n",
    "        ncdf_file_name = filepath.split('/')[-1]\n",
    "        if ftp_host.path.isfile(filepath):\n",
    "            print('.....Downloading ' + ncdf_file_name)\n",
    "            cwd = os.getcwd()\n",
    "            os.chdir(output_directory)\n",
    "            try:\n",
    "                ftp_host.download(filepath, ncdf_file_name)  # remote, local\n",
    "                print('Done!')\n",
    "            except Exception as e:\n",
    "                print('error: FTP download is forbidden in the remote server...')\n",
    "            os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>TIP</b>\n",
    "    \n",
    "***  \n",
    "To check if the files have been donwloaded just check if you find them in the directory specified as <i>output directory</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>WARNING</b>\n",
    " \n",
    "***  \n",
    "FTP download is forbidden in the remote server for security reasons. <br>Please run this jupyter notebook locally for being able to actually download the files. Steps:\n",
    "1. [Install anaconda](https://www.anaconda.com/distribution/): according to your OS (Windows,Linux,Mac...)\n",
    "2. Run the following command at the Terminal (Mac/Linux) or Command Prompt (Windows): jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Wrap-up\n",
    "\n",
    "So far you should already know how to subset the product/dataset by several subsetting criterias as well as exporting and downloading the resulting subset of files.<br> `If you don't please ask us! it is the moment!`\n",
    "<br>In the next tutorial we will see how to open and visualize some of the files donloaded. Ready? Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Next Tutorial\n",
    "\n",
    "_Click on one of the hyperlinks below to continue the training_\n",
    "<br>\n",
    "[**13-03-NearRealTime-product-managing-files-moorings.ipynb**](13-03-NearRealTime-product-managing-files-moorings.ipynb)<br>\n",
    "[**13-04-NearRealTime-product-managing-files-profilers.ipynb**](13-04-NearRealTime-product-managing-files-profilers.ipynb)<br>\n",
    "[**13-05-NearRealTime-product-managing-files-thermosal.ipynb**](13-05-NearRealTime-product-managing-files-thermosal.ipynb)<br>"
   ]
  }
 ],
 "metadata": {
  "finalized": {
   "timestamp": 1572899020604,
   "trusted": true
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
